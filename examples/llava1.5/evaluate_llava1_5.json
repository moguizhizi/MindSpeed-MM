{
  "model_id": "llava",
  "text_decoder": {
    "num_layers": 32,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "num_query_groups": 32,
    "ffn_hidden_size": 11008,
    "add_bias_linear": false,
    "bias_activation_fusion": false,
    "gated_linear_unit": true,
    "apply_query_key_layer_scaling": false,
    "layernorm_zero_centered_gamma": false,
    "max_position_embeddings": 4096,
    "bias_dropout_fusion": false,
    "apply_rope_fusion": false,
    "attention_softmax_in_fp32": true,
    "attention_dropout": 0.0,
    "hidden_dropout": 0.0,
    "fp16": true,
    "params_dtype": "fp16",
    "deallocate_pipeline_outputs": true,
    "persist_layer_norm": true,
    "activation_func": "silu",
    "normalization": "RMSNorm",
    "language_vocab_size": 32000,
    "language_max_sequence_length": 4096,
    "lm_position_embedding_type": "rope",
    "is_encoder_decoder": false,
    "freeze": true,
    "ckpt_path": "/<your_vicuna_weights_path>/converted_vicuna.pt"
  },
  "image_encoder": {
    "vision_encoder": {
      "model_id": "clip",
      "num_layers": 24,
      "hidden_size": 1024,
      "num_attention_heads": 16,
      "num_query_groups": 16,
      "ffn_hidden_size": 4096,
      "post_layer_norm": false,
      "add_bias_linear": true,
      "add_qkv_bias": true,
      "hidden_dropout": 0.0,
      "attention_dropout": 0.0,
      "fp16": true,
      "params_dtype": "fp16",
      "gated_linear_unit": false,
      "kv_channels": 64,
      "layernorm_zero_centered_gamma": false,
      "bias_activation_fusion": false,
      "bias_dropout_fusion": false,
      "attention_softmax_in_fp32": true,
      "normalization": "LayerNorm",
      "apply_rope_fusion": false,
      "activation_func": "quick_gelu",
      "device": "npu",
      "add_class_token": true,
      "class_token_len": 1,
      "patch_size": 14,
      "image_size": 336,
      "freeze": true,
      "ckpt_path": "/<your_clip_weights_path>/converted_clip.pt"
    },
    "vision_projector": {
      "model_id": "mlp",
      "num_attention_heads": 1,
      "num_layers": 2,
      "gated_linear_unit": false,
      "bias_activation_fusion": false,
      "add_bias_linear": true,
      "input_size": 1024,
      "hidden_size": 4096,
      "ffn_hidden_size": 4096,
      "bf16": true,
      "params_dtype": "fp16",
      "activation_func": "gelu",
      "freeze": true,
      "ckpt_path": "/<your_clip_weights_path>/converted_mlp.pt"
    }
  },
  "text_encoder": null,
  "video_encoder": null,
  "dtype": "fp16",
  "device": "npu",
  "tokenizer": {
    "hub_backend": "hf",
    "autotokenizer_name": "AutoTokenizer",
    "from_pretrained": "./llava_7b",
    "use_fast": false
  },
  "generation_config": {
    "bos_token_id": 1,
    "eos_token_id": 2,
    "max_length": 4096,
    "pad_token_id": 0,
    "temperature": 0.0,
    "max_new_tokens": 128,
    "output_attentions": false,
    "output_hidden_states": false,
    "use_cache": false,
    "decoder_start_token_id": null,
    "min_new_tokens": null,
    "min_length": 0,
    "constraints": null,
    "num_beams": 1,
    "do_sample": false,
    "force_words_ids": null,
    "top_k": 50,
    "top_p": null,
    "prompt_lookup_num_tokens": null,
    "guidance_scale": null,
    "bad_words_ids": null,
    "begin_suppress_tokens": null,
    "diversity_penalty": 0.0,
    "early_stopping": false,
    "encoder_no_repeat_ngram_size": 0,
    "encoder_repetition_penalty": 1.0,
    "epsilon_cutoff": 0.0,
    "eta_cutoff": 0.0,
    "exponential_decay_length_penalty": null,
    "forced_bos_token_id": null,
    "forced_decoder_ids": null,
    "forced_eos_token_id": null,
    "length_penalty": 1.0,
    "low_memory": null,
    "max_time": null,
    "no_repeat_ngram_size": 0,
    "num_assistant_tokens": 5,
    "num_assistant_tokens_schedule": "heuristic",
    "num_beam_groups": 1,
    "num_return_sequences": 1,
    "output_scores": false,
    "penalty_alpha": null,
    "remove_invalid_values": false,
    "renormalize_logits": false,
    "repetition_penalty": 1.0,
    "return_dict_in_generate": false,
    "sequence_bias": null,
    "suppress_tokens": null,
    "typical_p": 1.0
  },
  "dataset_path": "./AI2D_TEST.tsv",
  "evaluation_dataset": "ai2d_test",
  "evaluation_model": "llava_v1.5_7b",
  "result_output_path": "./evaluation_outputs/",
  "image_processer_path": "./llava_weights_mm/clip-vit-large-patch14-336"
}