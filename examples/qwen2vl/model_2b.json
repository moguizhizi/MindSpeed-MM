{
    "model_id": "qwen2vl",
    "img_context_token_id": 151655,
    "video_token_id": 151656,
    "vision_start_token_id": 151652,
    "image_encoder": {
        "vision_encoder": {
            "model_id": "qwen2vit",
            "num_layers": 32,
            "hidden_size": 1280,
            "ffn_hidden_size": 5120,
            "llm_hidden_size": 1536,
            "num_attention_heads": 16,
            "hidden_dropout": 0.0,
            "attention_dropout": 0.0,
            "in_channels": 3,
            "patch_size": 14,
            "spatial_merge_size": 2,
            "temporal_patch_size": 2,
            "layernorm_epsilon": 1e-06,
            "normalization": "LayerNorm",
            "fp16": false,
            "bf16": true,
            "params_dtype": "bf16",
            "activation_func": "quick_gelu",
            "freeze": true,
            "use_fused_rotary_pos_emb": true,
            "post_layer_norm": false,
            "pipeline_num_layers": [32]
        },
        "vision_projector": {
            "model_id": "lnmlp",
            "num_layers": 1,
            "num_attention_heads": 1,
            "gated_linear_unit": false,
            "bias_activation_fusion": false,
            "add_bias_linear": true,
            "input_size": 1280,
            "hidden_size": 1536,
            "ffn_hidden_size": 5120,
            "activation_func": "gelu",
            "bf16": true,
            "params_dtype": "bf16",
            "freeze": true,
            "layernorm_epsilon": 1e-06,
            "normalization": "LayerNorm"
        }
    },
    "text_decoder": {
        "model_id": "qwen2lm",
        "num_layers": 28,
        "pipeline_num_layers": [28],
        "hidden_size": 1536,
        "ffn_hidden_size": 8960,
        "num_attention_heads": 12,
        "max_position_embeddings": 32768,
        "vocab_size": 151936,
        "rope_theta": 1000000.0,
        "untie_embeddings_and_output_weights": false,
        "disable_bias_linear": true,
        "attention_dropout": 0.0,
        "init_method_std": 0.01,
        "hidden_dropout": 0.0,
        "position_embedding_type": "mrope",
        "normalization": "RMSNorm",
        "activation_func": "silu",
        "use_fused_rotary_pos_emb": true,
        "attention_softmax_in_fp32": true,
        "params_dtype": "bf16",
        "bf16": true,
        "parallel_output": true,
        "group_query_attention": true,
        "num_query_groups": 2,
        "mrope_section": [16, 24, 24],
        "rope_scaling": null,
        "gated_linear_unit": true,
        "layernorm_epsilon": 1e-06,
        "add_bias_linear":false,
        "add_qkv_bias": true
    },
    "text_encoder": null,
    "video_encoder": null
}
