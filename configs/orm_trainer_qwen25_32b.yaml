defaults:
  - model:
      - qwen25_32b

reward:
  model: qwen25_32b
  global_batch_size: 64
  seq_length: 4096
  tokenizer_type: PretrainedFromHF
  tokenizer_name_or_path: /data/for_dt/tokenizer/Qwen25-7B
  train_iters: 200
  distributed_backend: nccl
  no_shared_storage: false
  no_load_optim: true
  no_load_rng: true
  variable_seq_lengths: false
  no_save_optim: true
  no_save_rng: true
  stage: orm
  prompt_type: qwen
  load_checkpoint_loosely: true
  sequence_parallel: True
  bf16: true
  lr: 1.25e-6
  lr-decay-style: cosine
  attention_dropout: 0.0
  hidden_dropout: 0.0
  init_method_std: 0.01
  weight_decay: 0.1
  lr_warmup_fraction: 0.01
  clip_grad: 1.0
  adam_beta1: 0.9
  adam_beta2: 0.95
  initial_loss_scale: 4096
  is_pairwise_dataset: true
  norm_epsilon: 1e-6
  finetune: true
  data_path: /data/for_dt/datasets/convert/orca_rlhf
  split: 12659,200,0
  log_interval: 1
  save_interval: 100
  eval_interval: 100
  eval_iters: 0
  no_gradient_accumulation_fusion: true
  no_masked_softmax_fusion: true
  use_distributed_optimizer: true
  use_fused_swiglu: true
  use_fused_rotary_pos_emb: true
  use_fused_rmsnorm: true
  tokenizer_not_use_fast: true
  use_mcore_models: true
  overlap_grad_reduce: true
  use_flash_attn: true
  use_rotary_position_embeddings: false
  rope_scaling_factor: 1.0
  rotary_percent: 1.0
  pad_to_multiple_of: 8
  load: ./ckpt
  save: ./ckpt
  micro_batch_size: 1
  tensor_model_parallel_size: 8
  pipeline_model_parallel_size: 2